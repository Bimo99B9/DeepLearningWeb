{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iibwyCW1iZ9T"
      },
      "source": [
        "## Introducción a PyTorch (Parte 7)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Introducci%C3%B3n%20a%20PyTorch%20(Parte%207).ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "En este notebook vamos a aprender como realizar un fine-tuning de un modelo. En este caso será un modelo de texto conocido como Bert. Puedes encontrar más información acerca de Bert en este [enlace](https://huggingface.co/docs/transformers/v4.30.0/en/model_doc/bert#transformers.BertForSequenceClassification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwhPwPE3iq5P"
      },
      "source": [
        "### Instalación de los paquetes necesarios para la ejecución del notebook\n",
        "En este caso necesitamos la librería transformers (Bert es un transformer) y la librería dataset para cargar el dataset que utilizaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aiGFG59iyzk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6rvasOgvJW7"
      },
      "source": [
        "### Carga del dataset\n",
        "El dataset a utilizar en este caso será SST2. El conjunto de datos SST-2 (Stanford Sentiment Treebank 2) es un conjunto de datos ampliamente utilizado para tareas de análisis de sentimientos en el procesamiento del lenguaje natural. Consiste en reseñas de películas en inglés y su objetivo es determinar si una reseña tiene un sentimiento positivo o negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRUZZ5hdiZ9W"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar el dataset\n",
        "dataset = load_dataset('sst')\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "print(train_dataset)\n",
        "print(val_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUTsLWJiZ9Z"
      },
      "source": [
        "### Carga, procesado y visualización de datos\n",
        "En este dataset la etiqueta es un número de real, que va de 0 a 1, según la opinión sea negativa (cercano a cero) o positiva (cercano a uno). Nosotros vamos a convertir este problema a un problema binario, considerando que las opiniones con más de 0.5, son positivas y viceversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnP-EPgYiZ9a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Creamos un dataset específico para nuestro problema\n",
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = sentences\n",
        "        self.encodings = tokenizer(sentences, truncation=True, padding=True)\n",
        "        self.labels = [1 if label>=0.5 else 0 for label in labels]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Para un elemento dado devolvemos las tres cosas que nos devuelve el tokenizer: input_ids, token_type_ids y attention_masks\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['label'] = torch.tensor(self.labels[idx])\n",
        "        item['sentence'] = self.sentences[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "base_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "#Creación de datasets y dataloders\n",
        "train_dataset = SST2Dataset(train_dataset['sentence'],train_dataset['label'], tokenizer=tokenizer)\n",
        "test_dataset = SST2Dataset(test_dataset['sentence'],test_dataset['label'], tokenizer=tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQuk8O5iZ9d"
      },
      "source": [
        "Como puedes ver en el código anterior, hemos creado un Dataset específico para procesar los datos de nuestro problema. En el constructor tokenizamos todas las frases del dataset. El tokenizador nos devuelve tres elementos, que posteriormente devolveremos cuando se nos pida un elemento del dataset:\n",
        "- input_ids: Son identificadores numéricos que representan los tokens de entrada después de ser procesados por el tokenizer.\n",
        "- token_type_ids: Son identificadores que indican la pertenencia de cada token a una cierta segmentación o \"tipo\" en modelos que manejan múltiples secuencias de entrada.\n",
        "- attention_masks: Es una máscara binaria que indica qué tokens deben ser atendidos (valor 1) y cuáles deben ser ignorados (valor 0) durante la atención del modelo.\n",
        "\n",
        "Además devolvemos:\n",
        "- label: la etiqueta del ejemplo\n",
        "- sentence: la frase original (no se necesita para el entrenamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA5R8McxiZ9e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count_0 = train_dataset.labels.count(0)\n",
        "count_1 = train_dataset.labels.count(1)\n",
        "\n",
        "# Plotting\n",
        "labels = ['Negativa', 'Positiva']\n",
        "counts = [count_0, count_1]\n",
        "\n",
        "plt.bar(labels, counts)\n",
        "plt.title('Distribución de las opiniones en el dataset (train)')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar algunos ejemplos de frases\n",
        "print(\"---> Ejemplos de frases en el dataset de train:\\n\")\n",
        "for i in range(10):\n",
        "    idx = torch.randint(0,len(train_dataset),(1,))\n",
        "    print(train_dataset[idx]['sentence'], \"[Opinión positiva]\" if train_dataset[idx]['label'].item() else \"[Opinión negativa]\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsdsFEt_iZ9g"
      },
      "source": [
        "### Carga del modelo base y fine-tuning\n",
        "\n",
        "Vamos ahora a ajustar este modelo de lenguaje a nuestro problema en concreto (también se le conoce como downstream task). Haremos un bucle de entrenamiento típico como hemos visto en prácticas anteriores.\n",
        "\n",
        "Si lo necesitásemos, podríamos ajustar con el atributo `requires_grad`, que parámetros ajustar y cuales congelar. Esto es útil cuando solo queremos ajustar ciertas capas sin cambiar otras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj2MBL0FiZ9h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "#Indicamos el número de etiquetas para que se pueda crear la última capa de la red\n",
        "model = BertForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando el dispositivo %s\" % device)\n",
        "\n",
        "#Opcional, para congelar todos los parámetros menos la última capa\n",
        "#for param in model.base_model.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        #Esta clase (BertForSequenceClassification) ya es capaz de calcular el\n",
        "        #loss directamente, por eso le pasamos las etiquetas también\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step%10==0:\n",
        "            print(\"Step %d/%d\" % (step,len(train_dataloader)))\n",
        "    print(\"Training Loss %0.2f\" % (epoch_loss/len(train_dataloader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ9-DkjLmCpF"
      },
      "source": [
        "### Inferencia\n",
        "Ahora que hemos entrenado el modelo podemos predecir el conjunto de test para ver que tal funciona:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn5ZHusImImL"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "correct_predictions=0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_labels = torch.argmax(predicted_probabilities, dim=1)\n",
        "        correct_predictions += (predicted_labels == labels).sum().item()\n",
        "print(\"Acierto: %.2f\" % (correct_predictions/len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TlI8EHzwLEj"
      },
      "source": [
        "Como puedes ver, con un simple finetuning y tan solo tres épocas conseguimos unos resultados más que acetables (obviamente, son muy mejorables)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOTQlIb6lKI9"
      },
      "source": [
        "### Ejercicios\n",
        "\n",
        "1. Que error daría la red sin hacer fine-tuning?\n",
        "2. Añade un bucle de validación y early stopping.\n",
        "3. Añade código necesario para monitorizar el error de entrenamiento y de validación.\n",
        "4. Añade el código necesario para salvar el mejor modelo entrenado.\n",
        "5. Cambia el código para convertir el código en un problema multiclase de tres clases. En este caso, las opiniones en el rango [0, 0.3] serán negativas, las opiniones en el rango (0.3,0.7) serán neutras y las opiniones en el rango [0.7,1] serán positivas.\n",
        "6. ¿Cuántos datos son necesarios para realizar un fine-tuning efectivo? Reduce el número de frases de entrenamiento y crea un estudio de cuantos datos harían falta para que nuestro sistema siguiese funcionando bien."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
