{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitorización de experimentos (Wandb)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Monitorizaci%C3%B3n%20de%20experimentos%20(Wandb).ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "La monitorización de experimentos es un aspecto muy importante en el aprendizaje profundo. Habitualmente probamos muchas configuraciones, modelos, etc. siendo totalmente necesario tener una herramienta que sea capaz de llevar cuenta de estos experimentos realizados y de sus resultados.\n",
    "\n",
    "Una de estas herramientas es **Weights and Biases**. Weights and Biases nos permite tener un panel colaborativo donde almacenar todos nuestros experimentos. \n",
    "\n",
    "Lo primero es ir a la página de [Weights and Biases](https://wandb.ai/site) y crear una cuenta (es gratuito)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de los paquetes necesarios\n",
    "El uso de Weights and Biases es muy sencillo. Solo requiere la instalación de un paquete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/pablo/.local/lib/python3.10/site-packages (0.15.5)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (1.28.1)\n",
      "Requirement already satisfied: pathtools in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/pablo/.local/lib/python3.10/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/pablo/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/pablo/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pablo/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/pablo/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pablo/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/pablo/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucles de entrenamiento y validación usando wandb\n",
    "\n",
    "Realmente los bucles de entrenamiento y validación son los mismos que siempre, lo único que tendremos que intercalar ciertas instrucciones para loguear los resultados en la plataforma de wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_dataloader(is_train, batch_size, slice=5):\n",
    "    \"Get a training dataloader\"\n",
    "    full_dataset = torchvision.datasets.MNIST(root=\".\", train=is_train, transform=T.ToTensor(), download=True)\n",
    "    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))\n",
    "    loader = torch.utils.data.DataLoader(dataset=sub_dataset, batch_size=batch_size, shuffle=True if is_train else False, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "def get_model(dropout):\n",
    "    # Modelo simple solo para testear\n",
    "    model = nn.Sequential(nn.Flatten(),\n",
    "                         nn.Linear(28*28, 256),\n",
    "                         nn.BatchNorm1d(256),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout),\n",
    "                         nn.Linear(256,10)).to(device)\n",
    "    return model\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for i, (images, labels) in enumerate(valid_dl):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            if i==batch_idx and log_images:\n",
    "                log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
    "\n",
    "def log_image_table(images, predicted, labels, probs):\n",
    "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
    "    # 🐝 Create a wandb Table to log images, labels and predictions to\n",
    "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
    "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
    "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
    "    wandb.log({\"predictions_table\":table}, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):    \n",
    "    # Get the data\n",
    "    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
    "    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "    \n",
    "    # A simple MLP model\n",
    "    model = get_model(config.dropout)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "   # Training\n",
    "    example_ct = 0\n",
    "    step_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        for step, (images, labels) in enumerate(train_dl):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            example_ct += len(images)\n",
    "            metrics = {\"train/train_loss\": train_loss, \n",
    "                       \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                       \"train/example_ct\": example_ct}\n",
    "            \n",
    "            if step + 1 < n_steps_per_epoch:\n",
    "                # 🐝 Log train metrics to wandb \n",
    "                wandb.log(metrics)\n",
    "                \n",
    "            step_ct += 1\n",
    "\n",
    "        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))\n",
    "\n",
    "        # 🐝 Log train and validation metrics to wandb\n",
    "        val_metrics = {\"val/val_loss\": val_loss, \n",
    "                       \"val/val_accuracy\": accuracy}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # 🐝 Close your wandb run \n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzando diferentes experimentos\n",
    "\n",
    "En este ejemplo vamos a lanzar 5 experimentos diferentes con diferentes dropouts. Todos ellos quedarán logueados en nuestro dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpglez82\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pablo/Proyectos/DeepLearning/Prácticas/notebooks/wandb/run-20230714_130503-cpf8tkod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pglez82/test-wandb/runs/cpf8tkod' target=\"_blank\">run_0</a></strong> to <a href='https://wandb.ai/pglez82/test-wandb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pglez82/test-wandb' target=\"_blank\">https://wandb.ai/pglez82/test-wandb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pglez82/test-wandb/runs/cpf8tkod' target=\"_blank\">https://wandb.ai/pglez82/test-wandb/runs/cpf8tkod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.436, Valid Loss: 0.277615, Accuracy: 0.92\n",
      "Train Loss: 0.217, Valid Loss: 0.219783, Accuracy: 0.94\n",
      "Train Loss: 0.217, Valid Loss: 0.199790, Accuracy: 0.93\n",
      "Train Loss: 0.089, Valid Loss: 0.188260, Accuracy: 0.95\n",
      "Train Loss: 0.140, Valid Loss: 0.154381, Accuracy: 0.95\n",
      "Train Loss: 0.062, Valid Loss: 0.164541, Accuracy: 0.95\n",
      "Train Loss: 0.041, Valid Loss: 0.149887, Accuracy: 0.95\n",
      "Train Loss: 0.043, Valid Loss: 0.150042, Accuracy: 0.95\n",
      "Train Loss: 0.032, Valid Loss: 0.159821, Accuracy: 0.95\n",
      "Train Loss: 0.018, Valid Loss: 0.146578, Accuracy: 0.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b64ee1478cd46c1ba23c1cc78a98aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669162216688467, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for experiment in range(5):\n",
    "    # 🐝 initialise a wandb run\n",
    "    wandb.init(project=\"test-wandb\", name=\"run_{}\".format(experiment), config={\n",
    "            \"epochs\": 10,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 1e-3,\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            })\n",
    "    \n",
    "    # Copy your config \n",
    "    config = wandb.config\n",
    "    train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
