{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L_PPIxkwH6c"
      },
      "source": [
        "### Optimización de hiperparámetros (Optuna)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Optimizaci%C3%B3n%20de%20hiperparámetros%20(Optuna).ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "En el entrenamiento de redes neuronales profundas, existen una multitud de hiperparámetros que podemos optimizar. Algunos de los más importantes son los siguientes:\n",
        "- Learning rate\n",
        "- Momento\n",
        "- Tamaño del mini-batch\n",
        "- Weight decay\n",
        "- Cantidad de dropout\n",
        "- Optimizador utilizado\n",
        "- Número de capas de la red\n",
        "- Tamaño de las capas de la red (u otros parámetros de la capa como tamaño del kernel para CNNs, etc)\n",
        "- Funciones de activación usadas\n",
        "- etc.\n",
        "\n",
        "Es muy común que la búsqueda de hiperparámetros sea hecha de manera bastante artesanal, siguiendo la intuición y los conocimientos del científico de datos. Aún así, existe software que nos permite hacer esta búsqueda de manera más **sistemática y automática**. Uno de estos software es Optuna, que es el que veremos en esta práctica. Wandb también tiene su sistema de búsqueda de hiperparámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalación de los paquetes necesarios\n",
        "Para esta práctica necesitaremos instalar Optuna."
      ],
      "metadata": {
        "id": "TAkiPfJ8wP_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpBLl-HswWrZ",
        "outputId": "c1e4a33a-c78c-486d-ab58-4d59448cf925"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definición de la red\n",
        "\n",
        "Para este ejemplo vamos a partir de la red de una práctica anterior (la usada para el conjunto Fashion MNIST)."
      ],
      "metadata": {
        "id": "oqgWdgabwbki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, dropout=0.2, linear_sizes = (50, 50, 10)):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        previous_size = 28*28 # la entrada tienen que coincidir con el número de pixeles en la imagen\n",
        "        for i, linear_size in enumerate(linear_sizes):\n",
        "            self.layers.append(nn.Linear(previous_size, linear_size))\n",
        "            if i != len(linear_sizes):\n",
        "                # Añadir dropout salvo en la última capa de salida\n",
        "                self.layers.append(nn.Dropout(dropout))\n",
        "            previous_size = linear_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "PE9voObKwbP7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como puedes ver, el constructor de esta red ya nos permite alterar el dropout y el número y tamaño de capas lineales sin tener que modificar nada. Esto va a ser muy útil para el uso de optuna."
      ],
      "metadata": {
        "id": "Z7nHK8jewtxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de datos y creación de los dataloders\n",
        "\n",
        "Optuna va a tratar de buscar los mejores hiperparámetros tratando de optimizar una métrica concreta. Esto dependerá del problema en particular. En este caso trataremos de **optimizar el acierto sobre el conjunto de validación**."
      ],
      "metadata": {
        "id": "zcrDZmtKw-Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Utilizando dispositivo: %s\" % device)\n",
        "\n",
        "print(\"Datos de entrenamiento:\")\n",
        "print(training_data, end='\\n\\n')\n",
        "\n",
        "# Separación de un conjunto de validación\n",
        "training_data, validation_data = random_split(training_data,(50000,10000))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(validation_data, batch_size=64, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzIJYZuYxNxe",
        "outputId": "260b3432-2906-4700-b5ef-161fafde068f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando dispositivo: cpu\n",
            "Datos de entrenamiento:\n",
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definición de los bucles de entrenamiento y validación\n",
        "Estos bucles deben estar parametrizados para que cada entrenamiento use los hiperparámetros generados por Optuna"
      ],
      "metadata": {
        "id": "BuA4tNOA0gMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, loss_module, val_dataloader):\n",
        "    val_loss=0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data_inputs, data_labels in val_dataloader:\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            logits = model(data_inputs)\n",
        "            val_loss += loss_module(logits, data_labels)\n",
        "        return val_loss/ len(val_dataloader)\n",
        "\n",
        "def train(train_dataloader, val_dataloader, dropout, linear_sizes, learning_rate, epoch_callback):\n",
        "    # Creamos la red con los parámetros indicados por Optuna\n",
        "    model = Net(dropout = dropout, linear_sizes = linear_sizes)\n",
        "\n",
        "    # Definimos la función de pérdida\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Definimos el optimizador\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    min_loss = float('inf')\n",
        "    patience = 3\n",
        "    no_improvement = 0\n",
        "    # Training loop\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for data_inputs, data_labels in train_dataloader:\n",
        "            #Hacer una pasada hacia delante\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)  # Output is [Batch size, 1], but we want [Batch size]\n",
        "            #Calcular el valor de la función de pérdida para este mini-batch\n",
        "            loss = loss_module(preds, data_labels)\n",
        "            #Acumular el error (solo para luego mostrarlo)\n",
        "            epoch_loss += loss.item()\n",
        "            #Reiniciar los gradientes\n",
        "            optimizer.zero_grad()\n",
        "            #Pasada hacia atrás\n",
        "            loss.backward()\n",
        "            #Actualizar los parámetros\n",
        "            optimizer.step()\n",
        "        val_loss = validation(model, loss_module, val_dataloader)\n",
        "        epoch_callback(val_loss, epoch)\n",
        "        print(\"[Epoch %d] Training Loss %0.2f. Validation Loss %0.2f. Patience: %d/%d\" % (epoch, epoch_loss/len(train_dataloader), val_loss, no_improvement, patience))\n",
        "        if val_loss < min_loss:\n",
        "            min_loss = val_loss\n",
        "            no_improvement=0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        # parada temprana\n",
        "        if no_improvement>=patience:\n",
        "            return min_loss\n",
        "    return min_loss\n"
      ],
      "metadata": {
        "id": "lwzEHiER0rkl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de la función objetivo\n",
        "La función objetivo es la función que Optuna va a ejecutar en cada intento. En estos intentos Optuna calculará un conjunto de hiperparámetros basado en los hiperparámetros usados en intentos anteriores. Ten en cuenta que podríamos hacer una búsqueda exhaustiva de parámetros (estilo a un grid search), pero lo normal es hacer una búsqueda con algún tipo de heurístico que facilite la búsqueda de los mejores hiperparámetros. Luego hablaremos más de este tema."
      ],
      "metadata": {
        "id": "l_KUaXHyx9XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # learning rate\n",
        "    lr = trial.suggest_float(\"lr\", 0.00001, 0.01, log=True)\n",
        "\n",
        "    # Número y tamaño de las capas lineales\n",
        "    num_linear_layers = trial.suggest_int(\"number_of_layers\", 1, 3)\n",
        "    linear_sizes=[]\n",
        "    for i in range(num_linear_layers):\n",
        "        linear_sizes.append(trial.suggest_int(\"linear_sizes{}\".format(i), 1, 100))\n",
        "\n",
        "    dropout = trial.suggest_float(\"dropout\", 0, 0.5)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 2, 64)\n",
        "\n",
        "    parameters = {'lr': lr, 'num_linear_layers':num_linear_layers, 'linear_sizes': linear_sizes, 'dropout':dropout, 'batch_size':batch_size}\n",
        "\n",
        "    print(\"Empezando un nuevo intento con los siguientes parámetros:\",parameters)\n",
        "\n",
        "    #Esta función se llama al final de cada época y sirve para podar las ejecuciones menos prometedoras\n",
        "    def epoch_callback(loss, epoch):\n",
        "        trial.report(loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return train(train_dataloader, val_dataloader, dropout, linear_sizes, lr, epoch_callback)\n",
        ""
      ],
      "metadata": {
        "id": "0GIUKKFSyg8S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lanzamiento de la búsqueda de hiperparámetros"
      ],
      "metadata": {
        "id": "XDG8tqL718cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.pruners import HyperbandPruner\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "pruner = HyperbandPruner()\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"Fashion_mnist\",\n",
        "    storage=\"sqlite:///busqueda_hiperparametros.db\",\n",
        "    load_if_exists=True,\n",
        "    pruner=pruner,\n",
        ")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "GzReAbOZ2A4G",
        "outputId": "04432e45-fd94-4110-a449-b403c97770ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-07-14 10:14:57,771] Using an existing study with name 'Fashion_mnist' instead of creating a new one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empezando un nuevo intento con los siguientes parámetros: {'lr': 0.003179050447220693, 'num_linear_layers': 1, 'linear_sizes': [57], 'dropout': 0.3306677276296845, 'batch_size': 18}\n",
            "[Epoch 0] Training Loss 2.40. Validation Loss 1.55. Patience: 0/3\n",
            "[Epoch 1] Training Loss 1.96. Validation Loss 1.27. Patience: 0/3\n",
            "[Epoch 2] Training Loss 1.85. Validation Loss 1.13. Patience: 0/3\n",
            "[Epoch 3] Training Loss 1.77. Validation Loss 1.04. Patience: 0/3\n",
            "[Epoch 4] Training Loss 1.73. Validation Loss 0.97. Patience: 0/3\n",
            "[Epoch 5] Training Loss 1.70. Validation Loss 0.92. Patience: 0/3\n",
            "[Epoch 6] Training Loss 1.67. Validation Loss 0.88. Patience: 0/3\n",
            "[Epoch 7] Training Loss 1.67. Validation Loss 0.85. Patience: 0/3\n",
            "[Epoch 8] Training Loss 1.64. Validation Loss 0.82. Patience: 0/3\n",
            "[Epoch 9] Training Loss 1.62. Validation Loss 0.80. Patience: 0/3\n",
            "[Epoch 10] Training Loss 1.62. Validation Loss 0.78. Patience: 0/3\n",
            "[Epoch 11] Training Loss 1.59. Validation Loss 0.76. Patience: 0/3\n",
            "[Epoch 12] Training Loss 1.59. Validation Loss 0.74. Patience: 0/3\n",
            "[Epoch 13] Training Loss 1.59. Validation Loss 0.73. Patience: 0/3\n",
            "[Epoch 14] Training Loss 1.58. Validation Loss 0.72. Patience: 0/3\n",
            "[Epoch 15] Training Loss 1.57. Validation Loss 0.71. Patience: 0/3\n",
            "[Epoch 16] Training Loss 1.57. Validation Loss 0.70. Patience: 0/3\n",
            "[Epoch 17] Training Loss 1.56. Validation Loss 0.69. Patience: 0/3\n",
            "[Epoch 18] Training Loss 1.55. Validation Loss 0.68. Patience: 0/3\n",
            "[Epoch 19] Training Loss 1.56. Validation Loss 0.67. Patience: 0/3\n",
            "[Epoch 20] Training Loss 1.55. Validation Loss 0.67. Patience: 0/3\n",
            "[Epoch 21] Training Loss 1.55. Validation Loss 0.66. Patience: 0/3\n",
            "[Epoch 22] Training Loss 1.54. Validation Loss 0.65. Patience: 0/3\n",
            "[Epoch 23] Training Loss 1.54. Validation Loss 0.65. Patience: 0/3\n",
            "[Epoch 24] Training Loss 1.53. Validation Loss 0.64. Patience: 0/3\n",
            "[Epoch 25] Training Loss 1.53. Validation Loss 0.64. Patience: 0/3\n",
            "[Epoch 26] Training Loss 1.54. Validation Loss 0.63. Patience: 0/3\n",
            "[Epoch 27] Training Loss 1.52. Validation Loss 0.63. Patience: 0/3\n",
            "[Epoch 28] Training Loss 1.51. Validation Loss 0.62. Patience: 0/3\n",
            "[Epoch 29] Training Loss 1.51. Validation Loss 0.62. Patience: 0/3\n",
            "[Epoch 30] Training Loss 1.51. Validation Loss 0.61. Patience: 0/3\n",
            "[Epoch 31] Training Loss 1.50. Validation Loss 0.61. Patience: 0/3\n",
            "[Epoch 32] Training Loss 1.50. Validation Loss 0.61. Patience: 0/3\n",
            "[Epoch 33] Training Loss 1.49. Validation Loss 0.60. Patience: 0/3\n",
            "[Epoch 34] Training Loss 1.51. Validation Loss 0.60. Patience: 0/3\n",
            "[Epoch 35] Training Loss 1.51. Validation Loss 0.60. Patience: 0/3\n",
            "[Epoch 36] Training Loss 1.50. Validation Loss 0.59. Patience: 0/3\n",
            "[Epoch 37] Training Loss 1.50. Validation Loss 0.59. Patience: 0/3\n",
            "[Epoch 38] Training Loss 1.51. Validation Loss 0.59. Patience: 0/3\n",
            "[Epoch 39] Training Loss 1.50. Validation Loss 0.59. Patience: 0/3\n",
            "[Epoch 40] Training Loss 1.49. Validation Loss 0.58. Patience: 0/3\n",
            "[Epoch 41] Training Loss 1.50. Validation Loss 0.58. Patience: 0/3\n",
            "[Epoch 42] Training Loss 1.49. Validation Loss 0.58. Patience: 0/3\n",
            "[Epoch 43] Training Loss 1.48. Validation Loss 0.58. Patience: 0/3\n",
            "[Epoch 44] Training Loss 1.49. Validation Loss 0.58. Patience: 0/3\n",
            "[Epoch 45] Training Loss 1.49. Validation Loss 0.57. Patience: 1/3\n",
            "[Epoch 46] Training Loss 1.48. Validation Loss 0.57. Patience: 0/3\n",
            "[Epoch 47] Training Loss 1.48. Validation Loss 0.57. Patience: 0/3\n",
            "[Epoch 48] Training Loss 1.48. Validation Loss 0.56. Patience: 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-07-14 10:21:43,690] Trial 3 finished with value: 0.5617775321006775 and parameters: {'batch_size': 18, 'dropout': 0.3306677276296845, 'linear_sizes0': 57, 'lr': 0.003179050447220693, 'number_of_layers': 1}. Best is trial 3 with value: 0.5617775321006775.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 49] Training Loss 1.48. Validation Loss 0.56. Patience: 0/3\n",
            "Empezando un nuevo intento con los siguientes parámetros: {'lr': 0.0005563731383704818, 'num_linear_layers': 1, 'linear_sizes': [19], 'dropout': 0.2198276672290051, 'batch_size': 6}\n",
            "[Epoch 0] Training Loss 2.35. Validation Loss 1.90. Patience: 0/3\n",
            "[Epoch 1] Training Loss 1.84. Validation Loss 1.56. Patience: 0/3\n",
            "[Epoch 2] Training Loss 1.65. Validation Loss 1.39. Patience: 0/3\n",
            "[Epoch 3] Training Loss 1.53. Validation Loss 1.28. Patience: 0/3\n",
            "[Epoch 4] Training Loss 1.46. Validation Loss 1.20. Patience: 0/3\n",
            "[Epoch 5] Training Loss 1.41. Validation Loss 1.14. Patience: 0/3\n",
            "[Epoch 6] Training Loss 1.37. Validation Loss 1.09. Patience: 0/3\n",
            "[Epoch 7] Training Loss 1.33. Validation Loss 1.05. Patience: 0/3\n",
            "[Epoch 8] Training Loss 1.30. Validation Loss 1.02. Patience: 0/3\n",
            "[Epoch 9] Training Loss 1.29. Validation Loss 0.99. Patience: 0/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitorización del proceso\n",
        "Una herramienta muy útil para ver el proceso de Optuna es optuna-dashboard. Esta herramienta abre un interfaz web donde podemos ver el progreso de la búsqueda de hiperparámetros. Para lanzarla, necesitamos instalarla y dar acceso a la misma a la base de datos donde se guarda la información relativa a la búsqueda. En Google Colab, puede hacerse de esta manera:"
      ],
      "metadata": {
        "id": "smVlpGli4QFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-dashboard"
      ],
      "metadata": {
        "id": "rkqInYBm4iuC",
        "outputId": "6c45f2be-6f11-4215-9830-61d1c7f25b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-dashboard\n",
            "  Downloading optuna_dashboard-0.10.3-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bottle (from optuna-dashboard)\n",
            "  Downloading bottle-0.12.25-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.2/90.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optuna>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (23.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (1.2.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (1.22.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (2.0.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (6.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (3.1.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=2.4.0->optuna-dashboard) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (2.1.3)\n",
            "Installing collected packages: bottle, optuna-dashboard\n",
            "Successfully installed bottle-0.12.25 optuna-dashboard-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    import time\n",
        "    import threading\n",
        "    from optuna_dashboard import wsgi\n",
        "    import optuna\n",
        "    from wsgiref.simple_server import make_server\n",
        "\n",
        "\n",
        "    port = 1234\n",
        "    storage = optuna.storages.RDBStorage(\"sqlite:////content/busqueda_hiperparametros.db\")\n",
        "    app = wsgi(storage)\n",
        "    httpd = make_server(\"localhost\", port, app)\n",
        "    thread = threading.Thread(target=httpd.serve_forever)\n",
        "    thread.start()\n",
        "    time.sleep(3) # Wait until the server startup\n",
        "\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_window(port, path='/dashboard/')"
      ],
      "metadata": {
        "id": "AOc4p4m_4q9y",
        "outputId": "56010332-ec4a-4383-fe5c-3c73f9608066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(1234, \"/dashboard/\", \"https://localhost:1234/dashboard/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}